#Generate 10 orders and Order Quantity ( related to orders) files upload it to GCS. Make sure these files have customer and products from your master data.


----------->python3 generate.py


#Write a Python Cloud Function to be triggered on file upload to GCS and loads that data in BigQuery tables.

----> main.py
---->requirements.txt
google-cloud
google-cloud-bigquery

---->environment.yaml
BUCKET: mydukan_order_details
DATASET: mydukan_part2
VERSION: v14


gcloud beta functions deploy gcs_to_bigquery --runtime=python37 
--trigger-resource=gs://mydukan_order_details --trigger-event=google.storage.object.finalize --entry-point=csv_loader --env-vars-file=environment.yaml

--------------
There is currently no way to trigger Cloud Functions only for writes in a specific folder in Cloud Storage.
If you want to limit the triggering to a subset of the files in your project, putting them in a separate bucket is currently the only way to accomplish that.
----------------


#View the same data in your Data Studio Report and get the PDF

Create a view with required fields which we can import to datastudio

refer code----------->View.py

Set refreshness :15 min
