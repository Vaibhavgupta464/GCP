#Create an Avro file with 20K records and upload it on GCS bucket “firstname-lastname-composer” using python program. Use Service Accounts only.

•	The Avro binary format:
•	Is faster to load. The data can be read in parallel, even if the data blocks are compressed.
•	Doesn't require typing or serialization.
•	Is easier to parse because there are no encoding issues found in other formats such as ASCII.


pip3 install avro 
pip3 install fastavro

User.avsc

{"namespace": "me.adnansiddiqi",
 "type": "record",
 "name": "User",
 "fields": [
     {"name": "name", "type": "string"},
     {"name": "age",  "type": "int"}
 ]
} 

------> python3 Createavro.py

#Create a DAG to upload Avro file data from GCS bucket to a BigQuery table. Dataset name “dag-dataset” and table name “composer_data”

#to create table and dataset
------>python3 Cloud_composer_create_dataset_table.py

#to create  composer pipeline
---------->python3 composer_pipeline.py

gsutil cp composer_pipe.py gs://us-central1-vaibhav-compose-8c8a2522-bucket/dags
